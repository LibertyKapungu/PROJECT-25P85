{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f9c07e",
   "metadata": {},
   "source": [
    "# Export Best Model to ONNX Format\n",
    "This notebook demonstrates how to export the trained TinyGRUVAD model to the ONNX format for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83983d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the TinyGRUVAD model explicitly\n",
    "class TinyGRUVAD(nn.Module):\n",
    "    \"\"\"Light GRU-based VAD, causal, hearing-aid friendly (~3.5k params with 24 mel bands).\"\"\"\n",
    "    def __init__(self, input_dim=48, hidden_dim=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pre = nn.Conv1d(input_dim, input_dim, kernel_size=3, padding=0, groups=input_dim)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # x: (B,T,F)\n",
    "        x = x.transpose(1,2)              # (B,F,T)\n",
    "        # causal pad: pad (kernel_size-1) frames on the left only so conv doesn't see future frames\n",
    "        k = self.pre.kernel_size[0] if isinstance(self.pre.kernel_size, (list, tuple)) else self.pre.kernel_size\n",
    "        pad_left = k - 1\n",
    "        x = F.pad(x, (pad_left, 0))       # pad on time dimension (left, right)\n",
    "        x = self.pre(x).transpose(1,2)    # local causal conv\n",
    "        x = self.norm(x)\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.drop(out)\n",
    "        # return raw logits (B,T,1); use BCEWithLogitsLoss for stability\n",
    "        logits = self.fc(out)\n",
    "        return logits, h\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db80ec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Best model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the Best Model\n",
    "repo_root = Path.cwd().parent.parent  # Adjust path as needed\n",
    "model_path = repo_root / \"models\" / \"GRU_VAD\" / \"tiny_vad_best.pth\"\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = 48  # 24 mel bands + delta features\n",
    "hidden_dim = 16\n",
    "vad = TinyGRUVAD(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "vad.load_state_dict(torch.load(model_path, map_location=device))\n",
    "vad.eval()\n",
    "\n",
    "print(\"[INFO] Best model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cdee9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dummy Input for ONNX Export\n",
    "dummy_input = torch.randn(1, 100, input_dim).to(device)  # (Batch, Time, Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748bbdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapun_63wn2un\\AppData\\Local\\Temp\\ipykernel_32688\\2729632937.py:4: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:309: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4244: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:684: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:1154: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:309: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4244: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:684: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:1154: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:180.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model exported to ONNX format at: c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\models\\GRU_VAD\\tiny_vad_best.onnx\n"
     ]
    }
   ],
   "source": [
    "# Export the Model to ONNX Format\n",
    "onnx_path = repo_root / \"models\" / \"GRU_VAD\" / \"tiny_vad_best.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    vad, \n",
    "    dummy_input, \n",
    "    onnx_path, \n",
    "    export_params=True,  # Store the trained parameter weights inside the model file\n",
    "    opset_version=11,    # ONNX version\n",
    "    do_constant_folding=True,  # Optimize constant folding\n",
    "    input_names=['input'],   # Input tensor name\n",
    "    output_names=['output'], # Output tensor name\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size', 1: 'sequence_length'},  # Dynamic axes for batch and sequence length\n",
    "        'output': {0: 'batch_size', 1: 'sequence_length'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Model exported to ONNX format at: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9febaaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ONNX model is valid.\n"
     ]
    }
   ],
   "source": [
    "# Verify the ONNX Model\n",
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# Check the model\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"[INFO] ONNX model is valid.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984de17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 1: Imports & Setup ------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torchaudio, numpy as np, random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "repo_root = current_dir.parent.parent\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "print(\"Repo root:\", repo_root)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# helper utils\n",
    "from utils.audio_dataset_loader import (\n",
    "    load_ears_dataset, load_wham_dataset, load_noizeus_dataset,\n",
    "    create_audio_pairs, preprocess_audio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65834a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGRUVAD(nn.Module):\n",
    "    \"\"\"Light GRU-based VAD, causal, hearing-aid friendly (~2 k params).\"\"\"\n",
    "    def __init__(self, input_dim=32, hidden_dim=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pre = nn.Conv1d(input_dim, input_dim, kernel_size=3, padding=1, groups=input_dim)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # x: (B,T,F)\n",
    "        x = x.transpose(1,2)              # (B,F,T)\n",
    "        x = self.pre(x).transpose(1,2)    # local TDNN-like conv\n",
    "        x = self.norm(x)\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.drop(out)\n",
    "        p = self.sigmoid(self.fc(out))\n",
    "        return p, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 3: Load Datasets --------------------------\n",
    "max_pairs = 3000\n",
    "noise_files = load_wham_dataset(repo_root, mode=\"train\", max_files=max_pairs)\n",
    "clean_files = load_ears_dataset(repo_root, mode=\"train\")\n",
    "train_pairs = create_audio_pairs(noise_files, clean_files)\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "\n",
    "noise_val = load_wham_dataset(repo_root, mode=\"validation\", max_files=900)\n",
    "clean_val = load_ears_dataset(repo_root, mode=\"validation\")\n",
    "val_pairs = create_audio_pairs(noise_val, clean_val)\n",
    "print(f\"Val pairs: {len(val_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% ------------------------- Cell 4: Feature Extraction ---------------------\n",
    "def mix_and_extract(noisy_wave, clean_wave, fs, n_bands=16, frame_len=0.008, hop_len=0.004, device=device):\n",
    "    \"\"\"log-mel + delta features & frame-wise labels.\"\"\"\n",
    "    if noisy_wave.dim() > 1: noisy_wave=noisy_wave[0]\n",
    "    if clean_wave.dim() > 1: clean_wave=clean_wave[0]\n",
    "    n_fft, hop = int(fs*frame_len), int(fs*hop_len)\n",
    "    win = torch.hann_window(n_fft).to(device)\n",
    "\n",
    "    def pspec(w): \n",
    "        spec=torch.stft(w.to(device),n_fft,hop,window=win,return_complex=True)\n",
    "        return spec.abs()**2\n",
    "\n",
    "    mel = torchaudio.transforms.MelScale(n_mels=n_bands, sample_rate=fs, n_stft=n_fft//2+1).to(device)\n",
    "    melN = mel(pspec(noisy_wave)).clamp_min(1e-8)\n",
    "    melC = mel(pspec(clean_wave)).clamp_min(1e-8)\n",
    "    logN, logC = torch.log(melN.T+1e-8), torch.log(melC.T+1e-8)\n",
    "    delta=torch.zeros_like(logN); delta[1:]=logN[1:]-logN[:-1]\n",
    "    feats=torch.cat([logN,delta],1).unsqueeze(0)\n",
    "    ratio=melC.T.sum(1)/(melN.T.sum(1)+1e-8)\n",
    "    labels=(ratio>0.2).float().unsqueeze(1).unsqueeze(0)\n",
    "    return feats.to(device), labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% ------------------------- Cell 5: Dataset & Loader -----------------------\n",
    "class LiveMixDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,pairs,target_sr=16_000,snr_range=(-5,10),device=\"cpu\"):\n",
    "        self.pairs=pairs; self.sr=target_sr; self.range=snr_range; self.device=device\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self,idx):\n",
    "        n,c=self.pairs[idx]; snr=random.uniform(*self.range)\n",
    "        cw,nw,noisy,fs=preprocess_audio(Path(c),Path(n),self.sr,snr,None)\n",
    "        feats,labs=mix_and_extract(noisy,cw,fs,device=self.device)\n",
    "        return feats.squeeze(0),labs.squeeze(0)\n",
    "\n",
    "def collate_pad(batch):\n",
    "    feats,labs=zip(*batch); L=[f.size(0) for f in feats]; Tmax=max(L); Fdim=feats[0].size(1)\n",
    "    X=torch.zeros(len(batch),Tmax,Fdim); Y=torch.zeros(len(batch),Tmax,1)\n",
    "    for i,(f,l) in enumerate(zip(feats,labs)): X[i,:f.size(0)]=f; Y[i,:l.size(0)]=l\n",
    "    return X,Y,torch.tensor(L)\n",
    "\n",
    "train_ds=LiveMixDataset(train_pairs,device=device)\n",
    "val_ds=LiveMixDataset(val_pairs,device=device)\n",
    "train_dl=torch.utils.data.DataLoader(train_ds,batch_size=1,shuffle=True)\n",
    "val_dl=torch.utils.data.DataLoader(val_ds,batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 6: Training (clean live view) -------------------------------\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vad = TinyGRUVAD(32, 16).to(device)\n",
    "opt = torch.optim.AdamW(vad.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "crit = nn.BCELoss()\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=3)\n",
    "\n",
    "bestF1, bestPath = -1.0, repo_root / \"models\" / \"tiny_vad_best.pth\"\n",
    "bestPath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# prepare live plot\n",
    "plt.ion()\n",
    "fig, (ax_loss, ax_f1) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax_loss.set_title(\"Loss\"); ax_f1.set_title(\"Validation F1\")\n",
    "ax_loss.set_xlabel(\"Epoch\"); ax_f1.set_xlabel(\"Epoch\")\n",
    "ax_loss.set_ylabel(\"BCE Loss\"); ax_f1.set_ylabel(\"F1 Score\")\n",
    "ax_loss.grid(True, alpha=0.3); ax_f1.grid(True, alpha=0.3)\n",
    "train_losses, val_losses, f1_scores = [], [], []\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    # ---------------- Train ----------------\n",
    "    vad.train(); Ltr = 0.0\n",
    "    for x, y in tqdm(train_dl, desc=f\"Epoch {epoch}/50\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        p, _ = vad(x)\n",
    "        loss = crit(p, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        Ltr += loss.item()\n",
    "    Ltr /= len(train_dl)\n",
    "    train_losses.append(Ltr)\n",
    "\n",
    "    # ---------------- Validate ----------------\n",
    "    vad.eval(); Lval = 0.0; P, L = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            p, _ = vad(x)\n",
    "            Lval += crit(p, y).item()\n",
    "            P.append(p.cpu().numpy().ravel()); L.append(y.cpu().numpy().ravel())\n",
    "    Lval /= len(val_dl)\n",
    "    val_losses.append(Lval)\n",
    "\n",
    "    probs, labels = np.concatenate(P), np.concatenate(L).astype(int)\n",
    "    pred = (probs >= 0.5).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average=\"binary\", zero_division=0)\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    f1_scores.append(f1)\n",
    "    sched.step(Lval)\n",
    "\n",
    "    # ---------------- Save & Display ----------------\n",
    "    if f1 > bestF1:\n",
    "        bestF1 = f1\n",
    "        torch.save(vad.state_dict(), bestPath)\n",
    "        status = f\"[INFO] new best (F1={f1:.3f})\"\n",
    "    else:\n",
    "        status = \"\"\n",
    "\n",
    "    clear_output(wait=True)           # <-- clear console output\n",
    "    print(f\"Epoch {epoch}/50  TL={Ltr:.3f}  VL={Lval:.3f}  F1={f1:.3f}  AUC={auc:.3f}  {status}\")\n",
    "\n",
    "    # update live plots\n",
    "    ax_loss.cla(); ax_f1.cla()\n",
    "    ax_loss.plot(train_losses, \"b-\", label=\"Train Loss\")\n",
    "    ax_loss.plot(val_losses, \"r-\", label=\"Val Loss\")\n",
    "    ax_loss.legend(); ax_loss.set_xlabel(\"Epoch\"); ax_loss.set_ylabel(\"BCE Loss\"); ax_loss.grid(True)\n",
    "    ax_f1.plot(f1_scores, \"g-\", label=\"Val F1\")\n",
    "    ax_f1.legend(); ax_f1.set_xlabel(\"Epoch\"); ax_f1.set_ylabel(\"F1 Score\"); ax_f1.grid(True)\n",
    "    display(fig)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if sched._last_lr[0] < 1e-5: \n",
    "        print(\"[INFO] Early stop (triggered by LR floor)\")\n",
    "        break\n",
    "\n",
    "plt.ioff()\n",
    "vad.load_state_dict(torch.load(bestPath, map_location=device))\n",
    "print(f\"Loaded best model (F1={bestF1:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% ------------------------- Cell 8: Quick Validation Plot ------------------\n",
    "import matplotlib.pyplot as plt\n",
    "exN,exC=val_pairs[0]\n",
    "cw,nw,noisy,fs=preprocess_audio(Path(exC),Path(exN),16_000,0)\n",
    "# use 8 ms frames with 4 ms hop for the digital filter timing\n",
    "f,l=mix_and_extract(noisy,cw,fs,frame_len=0.008, hop_len=0.004)\n",
    "vad.eval(); \n",
    "with torch.no_grad(): p,_=vad(f.to(device))\n",
    "p=p.squeeze().cpu().numpy(); l=l.squeeze().cpu().numpy()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(p,label=\"P(speech)\"); plt.plot(l,label=\"Label\",alpha=.6); plt.legend(); plt.grid(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984de17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 1: Imports & Setup ------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torchaudio, numpy as np, random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "repo_root = current_dir.parent.parent\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "print(\"Repo root:\", repo_root)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# helper utils\n",
    "from utils.audio_dataset_loader import (\n",
    "    load_ears_dataset, load_wham_dataset, load_noizeus_dataset,\n",
    "    create_audio_pairs, preprocess_audio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 2: Global Plot Settings -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set global plot parameters\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "\n",
    "print(\"[INFO] Global plot settings applied: DPI=400, Font=Times New Roman 18pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65834a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGRUVAD(nn.Module):\n",
    "    \"\"\"Light GRU-based VAD, causal, hearing-aid friendly (~2 k params).\"\"\"\n",
    "    def __init__(self, input_dim=32, hidden_dim=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pre = nn.Conv1d(input_dim, input_dim, kernel_size=3, padding=0, groups=input_dim)\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # x: (B,T,F)\n",
    "        x = x.transpose(1,2)              # (B,F,T)\n",
    "        # causal pad: pad (kernel_size-1) frames on the left only so conv doesn't see future frames\n",
    "        k = self.pre.kernel_size[0] if isinstance(self.pre.kernel_size, (list, tuple)) else self.pre.kernel_size\n",
    "        pad_left = k - 1\n",
    "        x = F.pad(x, (pad_left, 0))       # pad on time dimension (left, right)\n",
    "        x = self.pre(x).transpose(1,2)    # local causal conv\n",
    "        x = self.norm(x)\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.drop(out)\n",
    "        # return raw logits (B,T,1); use BCEWithLogitsLoss for stability\n",
    "        logits = self.fc(out)\n",
    "        return logits, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 3: Load Datasets --------------------------\n",
    "max_pairs = 3000\n",
    "noise_files = load_wham_dataset(repo_root, mode=\"train\", max_files=max_pairs)\n",
    "clean_files = load_ears_dataset(repo_root, mode=\"train\")\n",
    "train_pairs = create_audio_pairs(noise_files, clean_files)\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "\n",
    "noise_val = load_wham_dataset(repo_root, mode=\"validation\", max_files=900)\n",
    "clean_val = load_ears_dataset(repo_root, mode=\"validation\")\n",
    "val_pairs = create_audio_pairs(noise_val, clean_val)\n",
    "print(f\"Val pairs: {len(val_pairs)}\")\n",
    "\n",
    "# Load TEST set (for final evaluation - never used during training)\n",
    "noise_test = load_wham_dataset(repo_root, mode=\"test\", max_files=900)\n",
    "clean_test = load_ears_dataset(repo_root, mode=\"test\")\n",
    "test_pairs = create_audio_pairs(noise_test, clean_test)\n",
    "print(f\"Test pairs: {len(test_pairs)}\")\n",
    "print(\"\\n [WARN] TEST SET: Only use for final evaluation, NOT during training/validation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_and_extract(noisy_wave, clean_wave, fs, n_bands=16, frame_len=0.008, hop_len=0.004):\n",
    "    \"\"\"Extract log-mel + delta features with realistic VAD labels from noisy audio.\n",
    "    \n",
    "    CRITICAL FIX: Uses noisy speech energy for labels (not clean/oracle).\n",
    "    This ensures train-test consistency - the model learns to detect speech\n",
    "    in the presence of noise, which matches real-world inference conditions.\n",
    "    \n",
    "    Frame timing: 8ms frame, 4ms hop (optimized for low-latency hearing-aid applications)\n",
    "    \n",
    "    IMPORTANT: use center=False in STFT to avoid adding future samples to each frame (causal framing).\n",
    "    \n",
    "    References:\n",
    "    - Sohn et al. (1999) - \"A Statistical Model-Based Voice Activity Detection\"\n",
    "    - Hughes & Mierle (2013) - \"Recurrent Neural Networks for Voice Activity Detection\"\n",
    "    - Graf et al. (2015) - \"Features for Voice Activity Detection: a Comparative Analysis\"\n",
    "    \"\"\"\n",
    "    if noisy_wave.dim() > 1: noisy_wave = noisy_wave[0]\n",
    "    if clean_wave.dim() > 1: clean_wave = clean_wave[0]\n",
    "    n_fft, hop = int(fs*frame_len), int(fs*hop_len)\n",
    "    win = torch.hann_window(n_fft)\n",
    "\n",
    "    def pspec(w): \n",
    "        # center=False -> causal framing (no future samples)\n",
    "        spec = torch.stft(w, n_fft, hop, window=win, center=False, return_complex=True)\n",
    "        return spec.abs()**2\n",
    "\n",
    "    mel = torchaudio.transforms.MelScale(n_mels=n_bands, sample_rate=fs, n_stft=n_fft//2+1)\n",
    "    \n",
    "    # Extract mel features from noisy audio (for VAD input)\n",
    "    melN = mel(pspec(noisy_wave)).clamp_min(1e-8)\n",
    "    logN = torch.log(melN.T + 1e-8)\n",
    "    delta = torch.zeros_like(logN)\n",
    "    delta[1:] = logN[1:] - logN[:-1]\n",
    "    feats = torch.cat([logN, delta], 1).unsqueeze(0)\n",
    "    \n",
    "    # REALISTIC LABELS: Use noisy speech energy (train-test consistency)\n",
    "    # This teaches the model to detect speech in noisy conditions, matching inference\n",
    "    noisy_energy = melN.T.sum(1)  # Sum across mel bands per frame\n",
    "    \n",
    "    # Adaptive threshold based on noisy signal statistics\n",
    "    # Use higher threshold (15%) to be more conservative with noise\n",
    "    energy_threshold = noisy_energy.mean() * 0.15\n",
    "    labels = (noisy_energy > energy_threshold).float().unsqueeze(1).unsqueeze(0)\n",
    "    \n",
    "    return feats, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% ------------------------- Cell 5: Dataset & Loader -----------------------\n",
    "class LiveMixDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,pairs,target_sr=16_000,snr_range=(-5,15)):\n",
    "        self.pairs=pairs; self.sr=target_sr; self.range=snr_range\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self,idx):\n",
    "        n,c=self.pairs[idx]; snr=random.uniform(*self.range)\n",
    "        cw,nw,noisy,fs=preprocess_audio(Path(c),Path(n),self.sr,snr,None)\n",
    "        feats,labs=mix_and_extract(noisy,cw,fs)\n",
    "        return feats.squeeze(0),labs.squeeze(0)\n",
    "\n",
    "def collate_pad(batch):\n",
    "    feats,labs=zip(*batch); L=[f.size(0) for f in feats]; Tmax=max(L); Fdim=feats[0].size(1)\n",
    "    X=torch.zeros(len(batch),Tmax,Fdim); Y=torch.zeros(len(batch),Tmax,1)\n",
    "    for i,(f,l) in enumerate(zip(feats,labs)): X[i,:f.size(0)]=f; Y[i,:l.size(0)]=l\n",
    "    return X,Y,torch.tensor(L)\n",
    "\n",
    "train_ds=LiveMixDataset(train_pairs)\n",
    "val_ds=LiveMixDataset(val_pairs)\n",
    "train_dl=torch.utils.data.DataLoader(train_ds,batch_size=8,shuffle=True,collate_fn=collate_pad,pin_memory=True)\n",
    "val_dl=torch.utils.data.DataLoader(val_ds,batch_size=8,collate_fn=collate_pad,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 6: Enhanced Training (Realistic Labels + Improvements) --------------------------\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize model\n",
    "vad = TinyGRUVAD(32, 16).to(device)\n",
    "opt = torch.optim.AdamW(vad.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "# Check class balance and set weighted loss - IMPROVED: Sample more batches\n",
    "print(\"Checking class balance in training data...\")\n",
    "sample_labels = []\n",
    "num_sample_batches = min(100, len(train_dl))  # Sample 100 batches or all if fewer\n",
    "print(f\"Sampling {num_sample_batches} batches to estimate class balance...\")\n",
    "\n",
    "for i, (_, y, lengths) in enumerate(train_dl):\n",
    "    if i >= num_sample_batches:\n",
    "        break\n",
    "    # Only collect valid (non-padded) labels using lengths\n",
    "    for b in range(y.size(0)):\n",
    "        valid_length = int(lengths[b].item())\n",
    "        sample_labels.append(y[b, :valid_length].numpy().ravel())\n",
    "\n",
    "# Concatenate all valid labels\n",
    "sample_labels_cat = np.concatenate(sample_labels)\n",
    "speech_ratio = sample_labels_cat.mean()\n",
    "print(f\"Training data speech ratio: {speech_ratio:.2%}\")\n",
    "print(f\"Non-speech ratio: {(1-speech_ratio):.2%}\")\n",
    "\n",
    "# Weighted loss to balance classes (Graf et al. 2015)\n",
    "pos_weight = torch.tensor([(1 - speech_ratio) / (speech_ratio + 1e-8)]).to(device)\n",
    "print(f\"Using pos_weight={pos_weight.item():.2f} for class balancing\\n\")\n",
    "\n",
    "crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='sum')\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=3)\n",
    "\n",
    "bestF1, bestPath = -1.0, repo_root / \"models\" / \"GRU_VAD\" / \"tiny_vad_best.pth\"\n",
    "bestPath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create directory for saving all best models per epoch\n",
    "models_dir = repo_root / \"models\" / \"GRU_VAD\" / \"training_checkpoints\"\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Checkpoint directory: {models_dir}\")\n",
    "\n",
    "# Create directory for saving training data/graphs\n",
    "training_data_dir = repo_root / \"results\" / \"GRU_VAD\" / \"training_data\"\n",
    "training_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Training data directory: {training_data_dir}\")\n",
    "\n",
    "# Early stopping configuration\n",
    "early_stop_patience = 10  # Stop if no improvement for 10 epochs\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# LR warmup function (Vaswani et al. 2017)\n",
    "def lr_warmup(epoch, warmup_epochs=5, base_lr=1e-3):\n",
    "    \"\"\"Linear warmup for first few epochs.\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        return base_lr * (epoch + 1) / warmup_epochs\n",
    "    return base_lr\n",
    "\n",
    "# Prepare live plotting\n",
    "plt.ion()\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "ax_loss, ax_f1, ax_acc, ax_bal = axes.flatten()\n",
    "\n",
    "ax_loss.set_title(\"Loss\"); ax_loss.set_xlabel(\"Epoch\"); ax_loss.set_ylabel(\"BCE Loss\"); ax_loss.grid(True, alpha=0.3)\n",
    "ax_f1.set_title(\"F1 / Precision / Recall\"); ax_f1.set_xlabel(\"Epoch\"); ax_f1.set_ylabel(\"Score\"); ax_f1.grid(True, alpha=0.3)\n",
    "ax_acc.set_title(\"Accuracy / Specificity\"); ax_acc.set_xlabel(\"Epoch\"); ax_acc.set_ylabel(\"Score\"); ax_acc.grid(True, alpha=0.3)\n",
    "ax_bal.set_title(\"Class Balance\"); ax_bal.set_xlabel(\"Epoch\"); ax_bal.set_ylabel(\"Proportion\"); ax_bal.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics storage\n",
    "train_losses, val_losses = [], []\n",
    "f1_scores, prec_scores, rec_scores = [], [], []\n",
    "acc_scores, spec_scores = [], []\n",
    "pred_speech_ratios, true_speech_ratios = [], []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENHANCED TRAINING: Realistic Labels + Gradient Clipping + LR Warmup + Class Balance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    # Apply LR warmup\n",
    "    current_lr = lr_warmup(epoch, warmup_epochs=5)\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "    \n",
    "    # ---------------- Train ----------------\n",
    "    vad.train()\n",
    "    Ltr = 0.0\n",
    "    for x, y, L in tqdm(train_dl, desc=f\"Epoch {epoch}/50\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, _ = vad(x)\n",
    "        \n",
    "        # Mask padded frames\n",
    "        B, Tmax = logits.size(0), logits.size(1)\n",
    "        mask = (torch.arange(Tmax, device=L.device)[None, :] < L[:, None]).to(device)\n",
    "        logits_flat = logits.squeeze(-1)[mask]\n",
    "        labels_flat = y.squeeze(-1)[mask]\n",
    "        \n",
    "        loss = crit(logits_flat, labels_flat) / mask.sum().clamp_min(1.0)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # GRADIENT CLIPPING (Pascanu et al. 2013 - prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(vad.parameters(), max_norm=1.0)\n",
    "        \n",
    "        opt.step()\n",
    "        Ltr += loss.item()\n",
    "    \n",
    "    Ltr /= len(train_dl)\n",
    "    train_losses.append(Ltr)\n",
    "    \n",
    "    # ---------------- Validate ----------------\n",
    "    vad.eval()\n",
    "    Lval = 0.0\n",
    "    P, L_all = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y, lengths in val_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = vad(x)\n",
    "            \n",
    "            # Masked validation loss\n",
    "            B, Tmax = logits.size(0), logits.size(1)\n",
    "            mask = (torch.arange(Tmax, device=lengths.device)[None, :] < lengths[:, None]).to(device)\n",
    "            logits_flat = logits.squeeze(-1)[mask]\n",
    "            labels_flat = y.squeeze(-1)[mask]\n",
    "            \n",
    "            Lval += (nn.functional.binary_cross_entropy_with_logits(\n",
    "                logits_flat, labels_flat, reduction='sum'\n",
    "            ) / mask.sum().clamp_min(1.0)).item()\n",
    "            \n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            labs = y.cpu().numpy()\n",
    "            \n",
    "            # Collect per-frame predictions\n",
    "            for b in range(B):\n",
    "                valid = slice(0, int(lengths[b].item()))\n",
    "                P.append(probs[b, valid].ravel())\n",
    "                L_all.append(labs[b, valid].ravel())\n",
    "    \n",
    "    Lval /= len(val_dl)\n",
    "    val_losses.append(Lval)\n",
    "    \n",
    "    # Compute metrics\n",
    "    probs = np.concatenate(P)\n",
    "    labels = np.concatenate(L_all).astype(int)\n",
    "    pred = (probs >= 0.5).astype(int)\n",
    "    \n",
    "    # Standard metrics\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average=\"binary\", zero_division=0)\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    \n",
    "    # ENHANCED METRICS\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    \n",
    "    # Specificity (true negative rate)\n",
    "    tn = ((pred == 0) & (labels == 0)).sum()\n",
    "    fp = ((pred == 1) & (labels == 0)).sum()\n",
    "    specificity = tn / (tn + fp + 1e-8)\n",
    "    \n",
    "    # Class balance monitoring\n",
    "    pred_speech_ratio = pred.mean()\n",
    "    true_speech_ratio = labels.mean()\n",
    "    \n",
    "    # Store metrics\n",
    "    f1_scores.append(f1)\n",
    "    prec_scores.append(prec)\n",
    "    rec_scores.append(rec)\n",
    "    acc_scores.append(acc)\n",
    "    spec_scores.append(specificity)\n",
    "    pred_speech_ratios.append(pred_speech_ratio)\n",
    "    true_speech_ratios.append(true_speech_ratio)\n",
    "    \n",
    "    # Update scheduler\n",
    "    sched.step(Lval)\n",
    "    \n",
    "    # ---------------- Save Best Model ----------------\n",
    "    if f1 > bestF1:\n",
    "        bestF1 = f1\n",
    "        torch.save(vad.state_dict(), bestPath)\n",
    "        \n",
    "        # Save checkpoint with epoch number\n",
    "        checkpoint_path = models_dir / f\"tiny_vad_epoch_{epoch:03d}_f1_{f1:.4f}.pth\"\n",
    "        torch.save(vad.state_dict(), checkpoint_path)\n",
    "        \n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        status = f\"[INFO] NEW BEST (F1={f1:.3f}) - Saved epoch {epoch}\"\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        status = f\"[DEBUG] (No improvement: {epochs_without_improvement}/{early_stop_patience})\"\n",
    "    \n",
    "    # ---------------- Display ----------------\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch {epoch:2d}/50  LR={current_lr:.1e}  TL={Ltr:.3f}  VL={Lval:.3f}\")\n",
    "    print(f\"  PREC={prec:.3f}  REC={rec:.3f}  F1={f1:.3f}  ACC={acc:.3f}\")\n",
    "    print(f\"  SPEC={specificity:.3f}  AUC={auc:.3f}  {status}\")\n",
    "    print(f\"  Pred Speech: {pred_speech_ratio:.2%}  True Speech: {true_speech_ratio:.2%}\")\n",
    "    \n",
    "    # Update plots\n",
    "    for ax in axes.flatten():\n",
    "        ax.cla()\n",
    "    \n",
    "    # Loss plot\n",
    "    ax_loss.plot(train_losses, \"b-\", label=\"Train Loss\", linewidth=2)\n",
    "    ax_loss.plot(val_losses, \"r-\", label=\"Val Loss\", linewidth=2)\n",
    "    ax_loss.legend(); ax_loss.set_xlabel(\"Epoch\"); ax_loss.set_ylabel(\"BCE Loss\")\n",
    "    ax_loss.set_title(\"Loss\"); ax_loss.grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1/Precision/Recall plot\n",
    "    ax_f1.plot(f1_scores, \"g-\", label=\"F1\", linewidth=2)\n",
    "    ax_f1.plot(prec_scores, \"c--\", label=\"Precision\", linewidth=1.5)\n",
    "    ax_f1.plot(rec_scores, \"m--\", label=\"Recall\", linewidth=1.5)\n",
    "    ax_f1.legend(); ax_f1.set_xlabel(\"Epoch\"); ax_f1.set_ylabel(\"Score\")\n",
    "    ax_f1.set_ylim(-0.05, 1.05); ax_f1.set_title(\"F1 / Precision / Recall\")\n",
    "    ax_f1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy/Specificity plot\n",
    "    ax_acc.plot(acc_scores, \"b-\", label=\"Accuracy\", linewidth=2)\n",
    "    ax_acc.plot(spec_scores, \"orange\", linestyle=\"--\", label=\"Specificity\", linewidth=1.5)\n",
    "    ax_acc.legend(); ax_acc.set_xlabel(\"Epoch\"); ax_acc.set_ylabel(\"Score\")\n",
    "    ax_acc.set_ylim(-0.05, 1.05); ax_acc.set_title(\"Accuracy / Specificity\")\n",
    "    ax_acc.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Class balance plot\n",
    "    ax_bal.plot(pred_speech_ratios, \"r-\", label=\"Predicted Speech %\", linewidth=2)\n",
    "    ax_bal.plot(true_speech_ratios, \"g--\", label=\"True Speech %\", linewidth=1.5)\n",
    "    ax_bal.legend(); ax_bal.set_xlabel(\"Epoch\"); ax_bal.set_ylabel(\"Proportion\")\n",
    "    ax_bal.set_ylim(-0.05, 1.05); ax_bal.set_title(\"Class Balance\")\n",
    "    ax_bal.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "    # Early stopping (monitor validation F1 improvement)\n",
    "    if epochs_without_improvement >= early_stop_patience:\n",
    "        print(f\"\\n[INFO] Early stopping: No F1 improvement for {early_stop_patience} epochs\")\n",
    "        print(f\"       Best F1: {bestF1:.3f} (saved at epoch {epoch - early_stop_patience})\")\n",
    "        break\n",
    "    \n",
    "    # Secondary stopping: LR floor (prevents infinite training if patience is too high)\n",
    "    if sched._last_lr[0] < 1e-5:\n",
    "        print(f\"\\n[INFO] LR floor reached: Stopping training\")\n",
    "        break\n",
    "\n",
    "# Save all training metrics to CSV for later analysis\n",
    "import pandas as pd\n",
    "print(\"\\n[INFO] Saving training metrics to CSV...\")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses) + 1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'f1_score': f1_scores,\n",
    "    'precision': prec_scores,\n",
    "    'recall': rec_scores,\n",
    "    'accuracy': acc_scores,\n",
    "    'specificity': spec_scores,\n",
    "    'pred_speech_ratio': pred_speech_ratios,\n",
    "    'true_speech_ratio': true_speech_ratios\n",
    "})\n",
    "\n",
    "metrics_csv_path = training_data_dir / \"training_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "print(f\"[INFO] Metrics saved to: {metrics_csv_path}\")\n",
    "\n",
    "# Save final training plot\n",
    "fig.savefig(training_data_dir / \"training_curves.png\", dpi=400, bbox_inches='tight')\n",
    "print(f\"[INFO] Training curves saved to: {training_data_dir / 'training_curves.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ENHANCED TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Best Validation F1: {bestF1:.3f}\")\n",
    "print(f\"  Model saved: {bestPath}\")\n",
    "print(f\"  Checkpoints saved in: {models_dir}\")\n",
    "print(f\"  Training data saved in: {training_data_dir}\")\n",
    "\n",
    "print(\"\\nKey Improvements Applied:\")\n",
    "print(\"   Realistic VAD labels (noisy energy - train/test consistency)\")\n",
    "print(\"   Enhanced metrics (Acc, Spec, Balance)\")\n",
    "print(\"   Class-balanced loss (sampled 100 batches)\")\n",
    "print(\"   Low-latency frame timing (8ms frame, 4ms hop)\")\n",
    "print(\"   Gradient clipping (max_norm=1.0)\")\n",
    "print(\"   LR warmup (5 epochs)\")\n",
    "\n",
    "# Load best model\n",
    "vad.load_state_dict(torch.load(bestPath, map_location=device))\n",
    "print(f\"  Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ====================== Cell 7: FINAL TEST SET EVALUATION ======================\n",
    "# IMPORTANT: Run this cell ONLY ONCE after training is complete!\n",
    "# This provides an unbiased estimate of model performance on held-out data.\n",
    "# \n",
    "# Reference: Goodfellow et al. (2016) \"Deep Learning\" - Section 5.3\n",
    "# \"The test set should be used only once, to evaluate the final chosen model.\"\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" [INFO] FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[INFO] Creating test dataset...\")\n",
    "\n",
    "# Create test dataset (never seen during training/validation)\n",
    "test_ds = LiveMixDataset(test_pairs)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=8, collate_fn=collate_pad, pin_memory=True)\n",
    "\n",
    "print(f\"[INFO] Test set size: {len(test_pairs)} pairs\")\n",
    "print(f\"[INFO] Evaluating best model (F1={bestF1:.3f} on validation)...\\n\")\n",
    "\n",
    "# Ensure model is in eval mode and using best weights\n",
    "vad.eval()\n",
    "vad.load_state_dict(torch.load(bestPath, map_location=device))\n",
    "\n",
    "# Collect predictions\n",
    "test_probs_list, test_labels_list = [], []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, lengths in tqdm(test_dl, desc=\"Testing\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, _ = vad(x)\n",
    "        \n",
    "        # Compute masked loss\n",
    "        B, Tmax = logits.size(0), logits.size(1)\n",
    "        mask = (torch.arange(Tmax, device=lengths.device)[None, :] < lengths[:, None]).to(device)\n",
    "        logits_flat = logits.squeeze(-1)[mask]\n",
    "        labels_flat = y.squeeze(-1)[mask]\n",
    "        \n",
    "        test_loss += (nn.functional.binary_cross_entropy_with_logits(\n",
    "            logits_flat, labels_flat, reduction='sum'\n",
    "        ) / mask.sum().clamp_min(1.0)).item()\n",
    "        \n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        labs = y.cpu().numpy()\n",
    "        \n",
    "        # Collect per-frame predictions (only valid frames)\n",
    "        for b in range(B):\n",
    "            valid_len = int(lengths[b].item())\n",
    "            test_probs_list.append(probs[b, :valid_len].ravel())\n",
    "            test_labels_list.append(labs[b, :valid_len].ravel())\n",
    "\n",
    "test_loss /= len(test_dl)\n",
    "\n",
    "# Concatenate all predictions\n",
    "test_probs_cat = np.concatenate(test_probs_list)\n",
    "test_labels_cat = np.concatenate(test_labels_list).astype(int)\n",
    "test_pred = (test_probs_cat >= 0.5).astype(int)\n",
    "\n",
    "# Compute comprehensive metrics\n",
    "test_prec, test_rec, test_f1, _ = precision_recall_fscore_support(\n",
    "    test_labels_cat, test_pred, average=\"binary\", zero_division=0\n",
    ")\n",
    "test_auc = roc_auc_score(test_labels_cat, test_probs_cat)\n",
    "test_acc = accuracy_score(test_labels_cat, test_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels_cat, test_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "test_specificity = tn / (tn + fp + 1e-8)\n",
    "test_sensitivity = tp / (tp + fn + 1e-8)  # Same as recall\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" [INFO] TEST SET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n  Loss:        {test_loss:.4f}\")\n",
    "print(f\"  Accuracy:    {test_acc:.4f}\")\n",
    "print(f\"  Precision:   {test_prec:.4f}\")\n",
    "print(f\"  Recall:      {test_rec:.4f}\")\n",
    "print(f\"  F1 Score:    {test_f1:.4f}\")\n",
    "print(f\"  Specificity: {test_specificity:.4f}\")\n",
    "print(f\"  AUC-ROC:     {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n  Speech frames predicted:     {test_pred.mean():.2%}\")\n",
    "print(f\"  Speech frames actual:        {test_labels_cat.mean():.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\" Detailed Classification Report\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(\n",
    "    test_labels_cat, test_pred, \n",
    "    target_names=['Non-Speech', 'Speech'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix (counts)\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar=True)\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "ax1.set_ylabel('True Label')\n",
    "ax1.set_title(f'Confusion Matrix (Test Set)\\nF1={test_f1:.3f}, Acc={test_acc:.3f}')\n",
    "ax1.set_xticklabels(['Non-Speech', 'Speech'])\n",
    "ax1.set_yticklabels(['Non-Speech', 'Speech'])\n",
    "\n",
    "# Confusion matrix (normalized)\n",
    "ax2 = axes[1]\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Greens', ax=ax2, cbar=True)\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "ax2.set_ylabel('True Label')\n",
    "ax2.set_title(f'Normalized Confusion Matrix\\nSpec={test_specificity:.3f}, Sens={test_sensitivity:.3f}')\n",
    "ax2.set_xticklabels(['Non-Speech', 'Speech'])\n",
    "ax2.set_yticklabels(['Non-Speech', 'Speech'])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix plot\n",
    "test_results_dir = repo_root / \"results\" / \"GRU_VAD\" / \"test_evaluation\"\n",
    "test_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "confusion_matrix_path = test_results_dir / \"confusion_matrix.png\"\n",
    "fig.savefig(confusion_matrix_path, dpi=400, bbox_inches='tight')\n",
    "print(f\"[INFO] Confusion matrix saved to: {confusion_matrix_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save test metrics to CSV\n",
    "import pandas as pd\n",
    "test_metrics_df = pd.DataFrame({\n",
    "    'metric': ['loss', 'accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'sensitivity', 'auc_roc', 'speech_pred_ratio', 'speech_true_ratio'],\n",
    "    'value': [test_loss, test_acc, test_prec, test_rec, test_f1, test_specificity, test_sensitivity, test_auc, test_pred.mean(), test_labels_cat.mean()]\n",
    "})\n",
    "test_metrics_path = test_results_dir / \"test_metrics.csv\"\n",
    "test_metrics_df.to_csv(test_metrics_path, index=False)\n",
    "print(f\"[INFO] Test metrics saved to: {test_metrics_path}\")\n",
    "\n",
    "# Save confusion matrix values\n",
    "cm_df = pd.DataFrame(cm, index=['Non-Speech', 'Speech'], columns=['Non-Speech', 'Speech'])\n",
    "cm_path = test_results_dir / \"confusion_matrix_values.csv\"\n",
    "cm_df.to_csv(cm_path)\n",
    "print(f\"[INFO] Confusion matrix values saved to: {cm_path}\")\n",
    "\n",
    "# Compare validation vs test performance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" [INFO] VALIDATION vs TEST COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Metric          Validation    Test       Delta\")\n",
    "print(f\"  {'â”€'*50}\")\n",
    "print(f\"  F1 Score        {bestF1:.4f}        {test_f1:.4f}     {(test_f1-bestF1):+.4f}\")\n",
    "print(f\"  Accuracy        N/A           {test_acc:.4f}\")\n",
    "print(f\"  AUC-ROC         N/A           {test_auc:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Interpretation\n",
    "if abs(test_f1 - bestF1) < 0.05:\n",
    "    print(\"\\n[INFO] Good generalization: Test F1 within 5% of validation F1\")\n",
    "elif test_f1 < bestF1 - 0.05:\n",
    "    print(\"\\n[WARNING] Slight overfitting: Test F1 lower than validation (may need regularization)\")\n",
    "else:\n",
    "    print(\"\\n[INFO] Test F1 exceeds validation! Model generalizes well.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" [INFO] TEST EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------- Cell 8: Quick Validation Visualization ------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Visualizing VAD predictions on a validation example...\")\n",
    "exN, exC = val_pairs[0]\n",
    "cw, nw, noisy, fs = preprocess_audio(Path(exC), Path(exN), 16_000, snr_db=0)\n",
    "\n",
    "# Extract features and labels using the same 8ms/4ms timing\n",
    "f, l = mix_and_extract(noisy, cw, fs, frame_len=0.008, hop_len=0.004)\n",
    "\n",
    "# Get model predictions\n",
    "vad.eval()\n",
    "with torch.no_grad():\n",
    "    logits, _ = vad(f.to(device))\n",
    "    p = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "    l = l.squeeze().cpu().numpy()\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "# Time axis\n",
    "time_axis = np.arange(len(p)) * 0.004  # 4ms hop\n",
    "\n",
    "# Plot 1: Predictions vs Labels\n",
    "ax1 = axes[0]\n",
    "ax1.plot(time_axis, p, 'b-', label=\"P(speech)\", linewidth=2)\n",
    "ax1.plot(time_axis, l, 'g--', label=\"Label\", alpha=0.6, linewidth=2)\n",
    "ax1.axhline(y=0.5, color='r', linestyle=':', alpha=0.5, label='Threshold')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.set_title('VAD Predictions vs Ground Truth Labels')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Plot 2: Binary Decisions\n",
    "ax2 = axes[1]\n",
    "pred_binary = (p >= 0.5).astype(int)\n",
    "ax2.fill_between(time_axis, 0, pred_binary, alpha=0.3, color='blue', label='Predicted Speech')\n",
    "ax2.fill_between(time_axis, 0, l, alpha=0.3, color='green', label='True Speech')\n",
    "ax2.set_ylabel('Speech Activity')\n",
    "ax2.set_title('Binary Speech Detection (Threshold = 0.5)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Plot 3: Errors\n",
    "ax3 = axes[2]\n",
    "errors = (pred_binary != l.astype(int)).astype(int)\n",
    "false_positives = ((pred_binary == 1) & (l == 0)).astype(int)\n",
    "false_negatives = ((pred_binary == 0) & (l == 1)).astype(int)\n",
    "\n",
    "ax3.fill_between(time_axis, 0, false_positives, alpha=0.5, color='red', label='False Positives')\n",
    "ax3.fill_between(time_axis, 0, false_negatives, alpha=0.5, color='orange', label='False Negatives')\n",
    "ax3.set_ylabel('Error Type')\n",
    "ax3.set_xlabel('Time (seconds)')\n",
    "ax3.set_title('Detection Errors')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save validation visualization\n",
    "val_viz_dir = repo_root / \"results\" / \"GRU_VAD\" / \"validation_visualization\"\n",
    "val_viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_viz_path = val_viz_dir / \"validation_example_vad.png\"\n",
    "fig.savefig(val_viz_path, dpi=400, bbox_inches='tight')\n",
    "print(f\"[INFO] Validation visualization saved to: {val_viz_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print frame-level statistics\n",
    "n_frames = len(p)\n",
    "n_errors = errors.sum()\n",
    "n_fp = false_positives.sum()\n",
    "n_fn = false_negatives.sum()\n",
    "frame_acc = 1 - (n_errors / n_frames)\n",
    "\n",
    "print(f\"\\nFrame-level Statistics:\")\n",
    "print(f\"  Total frames:       {n_frames}\")\n",
    "print(f\"  Errors:             {n_errors} ({n_errors/n_frames:.2%})\")\n",
    "print(f\"  False Positives:    {n_fp} ({n_fp/n_frames:.2%})\")\n",
    "print(f\"  False Negatives:    {n_fn} ({n_fn/n_frames:.2%})\")\n",
    "print(f\"  Frame Accuracy:     {frame_acc:.2%}\")\n",
    "\n",
    "# Save frame-level statistics\n",
    "import pandas as pd\n",
    "frame_stats_df = pd.DataFrame({\n",
    "    'metric': ['total_frames', 'errors', 'false_positives', 'false_negatives', 'frame_accuracy'],\n",
    "    'value': [n_frames, n_errors, n_fp, n_fn, frame_acc]\n",
    "})\n",
    "frame_stats_path = val_viz_dir / \"validation_frame_statistics.csv\"\n",
    "frame_stats_df.to_csv(frame_stats_path, index=False)\n",
    "print(f\"[INFO] Frame statistics saved to: {frame_stats_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

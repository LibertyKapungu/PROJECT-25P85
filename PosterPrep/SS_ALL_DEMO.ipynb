{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abd3b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mband_vad' from 'dsp_algorithms.mband' (c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\src\\dsp_algorithms\\mband.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeep_learning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgtcrn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GTCRN\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerate_and_save_spectrogram\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_and_save_spectrogram\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdsp_algorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmband\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mband_vad \u001b[38;5;28;01mas\u001b[39;00m mband\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'mband_vad' from 'dsp_algorithms.mband' (c:\\Users\\kapun_63wn2un\\Documents\\ELEN4012 - Investigation\\Repository\\PROJECT-25P85\\src\\dsp_algorithms\\mband.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#set random seeds for reproducibility\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "repo_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "model_path = repo_root / 'models' / 'GRU_VAD' /'tiny_vad_best.pth'\n",
    "\n",
    "from utils.audio_dataset_loader import (\n",
    "    load_ears_dataset,\n",
    "    load_noizeus_dataset,\n",
    "    create_audio_pairs,\n",
    "    preprocess_audio\n",
    ")\n",
    "from deep_learning.gtcrn import GTCRN\n",
    "from utils.generate_and_save_spectrogram import generate_and_save_spectrogram\n",
    "from dsp_algorithms.mband_var import mband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GTCRN model\n",
    "device = torch.device(\"cpu\")\n",
    "gtcrn_model = GTCRN().eval()\n",
    "ckpt_path = repo_root / \"src\" / \"deep_learning\" / \"gtcrn\" / \"gtcrn_main\" / \"checkpoints\" / \"model_trained_on_dns3.tar\"\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "gtcrn_model.load_state_dict(ckpt['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f82fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_path = repo_root / \"sound_data\" / \"raw\" / \"NOIZEUS_NOISE_DATASET\"\n",
    "babble_noise_path = noise_path / \"cafeteria_babble.wav\"\n",
    "street_noise_path = noise_path / \"Street Noise_downtown.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4554a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sound_path = repo_root / \"sound_data\" / \"raw\" / \"EARS_DATASET\" / \"p092\"\n",
    "clean_sound_file = clean_sound_path / \"emo_amazement_freeform.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3352320",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_dB = 0\n",
    "\n",
    "clean_waveform, noise_waveform, noisy_speech, clean_sr = preprocess_audio(\n",
    "    clean_speech=clean_sound_file, \n",
    "    noisy_audio=street_noise_path, \n",
    "    snr_db=snr_dB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e14c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_enh, ss_fs = mband(\n",
    "    noisy_audio=noisy_speech,\n",
    "    fs=clean_sr,\n",
    "    Nband=4,\n",
    "    Freq_spacing='linear',\n",
    "    FRMSZ=8,\n",
    "    OVLP=50,\n",
    "    AVRGING=1,\n",
    "    Noisefr=1,\n",
    "    FLOOR=0.002,\n",
    "    VAD=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTCRN inference\n",
    "input_stft = torch.stft(noisy_speech, 512, 256, 512, torch.hann_window(512).pow(0.5), return_complex=True)\n",
    "input_stft = torch.view_as_real(input_stft)  # Convert to (F, T, 2)\n",
    "with torch.no_grad():\n",
    "    output_stft = gtcrn_model(input_stft[None])[0]  # Add batch dimension: (1, F, T, 2)\n",
    "output_stft = torch.complex(output_stft[..., 0], output_stft[..., 1])  # Convert back to complex\n",
    "gtcrn_enhanced_speech = torch.istft(output_stft, 512, 256, 512, torch.hann_window(512).pow(0.5)).detach().cpu().numpy()\n",
    "\n",
    "ss_enh, ss_fs = mband(\n",
    "    noisy_audio=noisy_speech,\n",
    "    fs=clean_sr,\n",
    "    Nband=4,\n",
    "    Freq_spacing='linear',\n",
    "    FRMSZ=8,\n",
    "    OVLP=75,\n",
    "    AVRGING=1,\n",
    "    Noisefr=1,\n",
    "    FLOOR=0.7,\n",
    "    VAD=0,\n",
    ")\n",
    "\n",
    "gtcrn_enhanced_speech = ss_enh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f83936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SIDE-BY-SIDE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare audio signals for comparison\n",
    "noisy_compare = noisy_speech.clone().squeeze(0)\n",
    "mband_compare = torch.from_numpy(ss_enh).clone().squeeze(0)\n",
    "gtcrn_mband_compare = torch.from_numpy(gtcrn_enhanced_speech).clone().squeeze(0)\n",
    "clean_compare = clean_waveform.clone().squeeze(0)\n",
    "\n",
    "fs = 16000\n",
    "\n",
    "print(f\"\\nAudio lengths before trimming:\")\n",
    "print(f\"  noisy:              {len(noisy_compare)}\")\n",
    "print(f\"  MBAND:              {len(mband_compare)}\")\n",
    "print(f\"  GTCRN + MBAND:      {len(gtcrn_mband_compare)}\")\n",
    "print(f\"  clean:              {len(clean_compare)}\")\n",
    "\n",
    "# Trim to shortest length\n",
    "min_len = min(\n",
    "    len(noisy_compare), \n",
    "    len(mband_compare),\n",
    "    len(gtcrn_mband_compare),\n",
    "    len(clean_compare)\n",
    ")\n",
    "\n",
    "noisy_compare = noisy_compare[:min_len]\n",
    "mband_compare = mband_compare[:min_len]\n",
    "gtcrn_mband_compare = gtcrn_mband_compare[:min_len]\n",
    "clean_compare = clean_compare[:min_len]\n",
    "\n",
    "print(f\"\\nTrimmed to: {min_len} samples ({min_len/fs:.2f}s)\")\n",
    "\n",
    "# Create 4-panel comparison plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
    "time_axis = np.arange(min_len) / fs\n",
    "\n",
    "# 1. Noisy\n",
    "axes[0].plot(time_axis, noisy_compare.numpy(), 'r', alpha=0.7, linewidth=0.8)\n",
    "axes[0].set_title(f'1. Noisy Speech (SNR = {snr_dB}dB)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(0, min_len/fs)\n",
    "\n",
    "# 2. MBAND\n",
    "axes[1].plot(time_axis, mband_compare.numpy(), 'orange', alpha=0.7, linewidth=0.8)\n",
    "axes[1].set_title('2. MBAND', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, min_len/fs)\n",
    "\n",
    "# 3. GTCRN + MBAND\n",
    "axes[2].plot(time_axis, gtcrn_mband_compare.numpy(), 'b', alpha=0.7, linewidth=0.8)\n",
    "axes[2].set_title('3. GTCRN + MBAND', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Amplitude')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xlim(0, min_len/fs)\n",
    "\n",
    "# 4. Clean reference\n",
    "axes[3].plot(time_axis, clean_compare.numpy(), 'g', alpha=0.7, linewidth=0.8)\n",
    "axes[3].set_title('4. Clean Speech (Reference)', fontsize=12, fontweight='bold')\n",
    "axes[3].set_ylabel('Amplitude')\n",
    "axes[3].set_xlabel('Time (s)')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "axes[3].set_xlim(0, min_len/fs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Audio playback comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUDIO PLAYBACK COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"NOISY SPEECH (0dB SNR):\")\n",
    "display(ipd.Audio(noisy_compare.numpy(), rate=fs))\n",
    "\n",
    "print(\"\\nMBAND:\")\n",
    "display(ipd.Audio(mband_compare.numpy(), rate=fs))\n",
    "\n",
    "print(\"\\nGTCRN + MBAND:\")\n",
    "display(ipd.Audio(gtcrn_mband_compare.numpy(), rate=fs))\n",
    "\n",
    "print(\"\\nCLEAN SPEECH (Reference):\")\n",
    "display(ipd.Audio(clean_compare.numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the audio waveforms from earlier\n",
    "noisy_waveform = noisy_compare\n",
    "clean_waveform_trimmed = clean_compare\n",
    "mband_alone_waveform = mband_compare\n",
    "gtcrnmband_waveform = gtcrn_mband_compare\n",
    "sample_rate = 16000\n",
    "\n",
    "# Generate and save spectrograms using the utility function\n",
    "print(\"Generating and saving spectrograms...\")\n",
    "output_path = \"spectrograms/comparison\"\n",
    "\n",
    "generate_and_save_spectrogram(\n",
    "    waveform=clean_waveform_trimmed,\n",
    "    sample_rate=sample_rate,\n",
    "    output_image_path=output_path,\n",
    "    output_file_name=\"clean_speech_spectrogram\",\n",
    "    title=\"Clean Speech Mel Spectrogram\",\n",
    "    n_mels=128,\n",
    "    hop_length=512,\n",
    "    n_fft=1024,\n",
    "    colormap='plasma',\n",
    "    include_metadata_in_filename=False\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Image(filename=f'{output_path}/clean_speech_spectrogram.png'))\n",
    "\n",
    "generate_and_save_spectrogram(\n",
    "    waveform=noisy_waveform,\n",
    "    sample_rate=sample_rate,\n",
    "    output_image_path=output_path,\n",
    "    output_file_name=\"noisy_spectrogram\",\n",
    "    title=\"Noisy Speech Mel Spectrogram (Overlap of Noise and Speech)\",\n",
    "    n_mels=128,\n",
    "    hop_length=512,\n",
    "    n_fft=1024,\n",
    "    colormap='plasma',\n",
    "    include_metadata_in_filename=False\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Image(filename=f'{output_path}/noisy_spectrogram.png'))\n",
    "\n",
    "generate_and_save_spectrogram(\n",
    "    waveform=mband_alone_waveform,\n",
    "    sample_rate=sample_rate,\n",
    "    output_image_path=output_path,\n",
    "    output_file_name=\"mband_alone_spectrogram\",\n",
    "    title=\"MBAND Alone Mel Spectrogram\",\n",
    "    n_mels=128,\n",
    "    hop_length=512,\n",
    "    n_fft=1024,\n",
    "    colormap='plasma',\n",
    "    include_metadata_in_filename=False\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Image(filename=f'{output_path}/mband_alone_spectrogram.png'))\n",
    "\n",
    "generate_and_save_spectrogram(\n",
    "    waveform=gtcrnmband_waveform,\n",
    "    sample_rate=sample_rate,\n",
    "    output_image_path=output_path,\n",
    "    output_file_name=\"gtcrnmband_spectrogram\",\n",
    "    title=\"GTCRNMBAND Algorithm Mel Spectrogram\",\n",
    "    n_mels=128,\n",
    "    hop_length=512,\n",
    "    n_fft=1024,\n",
    "    colormap='plasma',\n",
    "    include_metadata_in_filename=False\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Image(filename=f'{output_path}/gtcrnmband_spectrogram.png'))\n",
    "\n",
    "print(\"Spectrograms generated, saved, and displayed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

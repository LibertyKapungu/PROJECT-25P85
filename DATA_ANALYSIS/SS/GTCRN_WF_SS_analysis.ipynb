{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTCRN vs Spectral Subtraction vs Wiener Filter - Fair Comparison\n",
    "# This notebook compares GTCRN, GTCRN+SS, GTCRN+WF on the SAME test set\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Configure paths\n",
    "repo_root = Path.cwd().parent.parent\n",
    "results_root = repo_root / \"results\" / \"EXP3\" /\"GTCRN\"\n",
    "figures_dir = repo_root / \"reports\" / \"figures\" / \"GTCRN_SS_WF_Comparison\"\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snr_levels = [-5, 0, 5, 10, 15]\n",
    "metrics_of_interest = [\"PESQ\", \"STOI\", \"SI_SDR\", \"DNSMOS_mos_ovr\"]  \n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,\n",
    "    \"font.size\": 14,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = {\n",
    "    \"Noisy\": {\n",
    "        \"label\": \"Noisy Baseline\",\n",
    "        \"color\": \"#1f77b4\",  # Blue\n",
    "        \"marker\": \"o\",\n",
    "        \"directory\": repo_root / \"results\" / \"BASELINE\" / \"NOIZEUS_EARS_BASELINE\",\n",
    "        \"template\": \"BASELINE_NOIZEUS_EARS_[{snr}]dB.csv\",\n",
    "    },\n",
    "    \"GTCRN\": {\n",
    "        \"label\": \"GTCRN\",\n",
    "        \"color\": \"#ff7e0ef3\",  # Orange\n",
    "        \"marker\": \"s\",\n",
    "        \"directory\": results_root / \"GTCRN\" / \"GTCRN_EXP3p2a_ss\",  \n",
    "        \"template\": \"GTCRN_EXP3p2a_merged_[{snr}]dB.csv\",  \n",
    "    },\n",
    "    \"GTCRN_WF\": {\n",
    "        \"label\": \"GTCRN_WF\",\n",
    "        \"color\": \"#2ca02c\",  # Green\n",
    "        \"marker\": \"^\",\n",
    "        \"directory\": results_root / \"GTCRN\" / \"GTCRNWF_BEST_CONFIG\", \n",
    "        \"template\": \"GTCRNWF_merged_[{snr}]dB.csv\",  \n",
    "    },\n",
    "    \"GTCRN_SS\": {\n",
    "        \"label\": \"mband_py_log_hybrid_20ms_ov75_fl0p8_nf1_N4\",\n",
    "        \"color\": \"#ff9900\",  # Yellow/Orange\n",
    "        \"marker\": \"D\",\n",
    "        \"directory\": results_root / \"spectral\" / \"Notebook_analysis\",  \n",
    "        \"template\": \"mband_py_log_hybrid_20ms_ov75_fl0p8_nf1_N4_[{snr}]dB.csv\",  \n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def load_experiment(prefix: str, meta: dict) -> pd.DataFrame:\n",
    "    \"\"\"Load merged CSV files for a given experiment across all SNR levels.\"\"\"\n",
    "    frames = []\n",
    "    directory = meta[\"directory\"]\n",
    "    template = meta[\"template\"]\n",
    "    \n",
    "    for snr in snr_levels:\n",
    "        csv_path = directory / template.format(snr=snr)\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            print(f\"⚠️  WARNING: File not found: {csv_path}\")\n",
    "            continue\n",
    "            \n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['SNR'] = snr\n",
    "        df['experiment'] = prefix\n",
    "        df['label'] = meta['label']\n",
    "        frames.append(df)\n",
    "    \n",
    "    if not frames:\n",
    "        raise FileNotFoundError(f\"No files found for {prefix}. Check paths in catalog.\")\n",
    "    \n",
    "    result = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    # Extract noise type if available\n",
    "    if 'enhanced_file' in result.columns:\n",
    "        result['noise_type'] = result['enhanced_file'].str.extract(r'NOIZEUS_NOISE_DATASET_(.*?)_SNR')\n",
    "    elif 'noisy_file' in result.columns:\n",
    "        result['noise_type'] = result['noisy_file'].str.extract(r'NOIZEUS_NOISE_DATASET_(.*?)_SNR')\n",
    "    else:\n",
    "        result['noise_type'] = np.nan\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def build_summary_tables() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return full concatenated data and SNR-aggregated summary tables.\"\"\"\n",
    "    all_frames = []\n",
    "    summaries = []\n",
    "    \n",
    "    for exp_name, meta in catalog.items():\n",
    "        print(f\"Loading {exp_name}...\")\n",
    "        df = load_experiment(exp_name, meta)\n",
    "        all_frames.append(df)\n",
    "\n",
    "        # Compute summary statistics per SNR\n",
    "        summary = df.groupby('SNR')[metrics_of_interest].mean().reset_index()\n",
    "        summary['experiment'] = exp_name\n",
    "        summary['label'] = meta['label']\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    full_df = pd.concat(all_frames, ignore_index=True)\n",
    "    summary_df = pd.concat(summaries, ignore_index=True)\n",
    "    \n",
    "    return full_df, summary_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Load Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading all experiments...\")\n",
    "full_results, summary_by_snr = build_summary_tables()\n",
    "\n",
    "print(\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"Total samples: {len(full_results)}\")\n",
    "print(f\"Experiments: {summary_by_snr['experiment'].unique()}\")\n",
    "print(f\"SNR levels: {sorted(summary_by_snr['SNR'].unique())}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nSummary by SNR (first 10 rows):\")\n",
    "display(summary_by_snr.head(10))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Overall Performance (Averaged Across All SNRs)\n",
    "# ============================================================================\n",
    "\n",
    "overall_mean = (\n",
    "    summary_by_snr.groupby(['experiment', 'label'])[metrics_of_interest]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values('DNSMOS_p808_mos', ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE (Averaged Across All SNRs)\")\n",
    "print(\"=\"*80)\n",
    "display(overall_mean)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Percentage Gains vs Reference Systems\n",
    "# ============================================================================\n",
    "\n",
    "def compute_percentage_gain(reference_key: str, reference_label: str) -> pd.DataFrame:\n",
    "    \"\"\"Return mean percentage gain over a reference experiment for all metrics.\"\"\"\n",
    "    reference = summary_by_snr[summary_by_snr['experiment'] == reference_key]\n",
    "    \n",
    "    rows = []\n",
    "    for exp_name in catalog:\n",
    "        if exp_name == reference_key:\n",
    "            continue\n",
    "        \n",
    "        exp_slice = summary_by_snr[summary_by_snr['experiment'] == exp_name]\n",
    "        merged = exp_slice.merge(reference, on='SNR', suffixes=('', '_ref'))\n",
    "        \n",
    "        for metric in metrics_of_interest:\n",
    "            delta = merged[metric] - merged[f\"{metric}_ref\"]\n",
    "            pct = (delta / merged[f\"{metric}_ref\"].abs()) * 100  # Handle negative values\n",
    "            rows.append({\n",
    "                'experiment': exp_name,\n",
    "                'label': merged['label'].iloc[0],\n",
    "                'metric': metric,\n",
    "                'avg_pct_gain': pct.mean(),\n",
    "            })\n",
    "    \n",
    "    pivot = pd.DataFrame(rows).pivot(index=['experiment', 'label'], columns='metric', values='avg_pct_gain')\n",
    "    pivot = pivot.reset_index()\n",
    "    return pivot.sort_values('DNSMOS_p808_mos', ascending=False)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERCENTAGE GAINS vs NOISY BASELINE\")\n",
    "print(\"=\"*80)\n",
    "pct_vs_noise = compute_percentage_gain('Noisy', 'Noisy')\n",
    "display(pct_vs_noise)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERCENTAGE GAINS vs GTCRN BASELINE\")\n",
    "print(\"=\"*80)\n",
    "pct_vs_gtcrn = compute_percentage_gain('GTCRN', 'GTCRN')\n",
    "display(pct_vs_gtcrn)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Best Configuration per Metric and SNR\n",
    "# ============================================================================\n",
    "\n",
    "best_records = []\n",
    "for snr in snr_levels:\n",
    "    subset = summary_by_snr[summary_by_snr['SNR'] == snr]\n",
    "    for metric in metrics_of_interest:\n",
    "        idx = subset[metric].idxmax()\n",
    "        row = subset.loc[idx]\n",
    "        best_records.append({\n",
    "            'SNR': snr,\n",
    "            'Metric': metric,\n",
    "            'Best Config': row['label'],\n",
    "            'Score': row[metric],\n",
    "        })\n",
    "\n",
    "best_table = pd.DataFrame(best_records)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST CONFIGURATION PER METRIC AND SNR\")\n",
    "print(\"=\"*80)\n",
    "display(best_table)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SNR-Specific Performance Tables\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE AT -5dB (Critical Low SNR)\")\n",
    "print(\"=\"*80)\n",
    "display(summary_by_snr[summary_by_snr['SNR'] == -5][['label'] + metrics_of_interest])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE AT 0dB\")\n",
    "print(\"=\"*80)\n",
    "display(summary_by_snr[summary_by_snr['SNR'] == 0][['label'] + metrics_of_interest])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE AT 5dB\")\n",
    "print(\"=\"*80)\n",
    "display(summary_by_snr[summary_by_snr['SNR'] == 5][['label'] + metrics_of_interest])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: Multi-SNR Performance Plot\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics_of_interest):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for exp_name, meta in catalog.items():\n",
    "        data = summary_by_snr[summary_by_snr['experiment'] == exp_name]\n",
    "        ax.plot(\n",
    "            data['SNR'], \n",
    "            data[metric],\n",
    "            label=meta['label'],\n",
    "            color=meta['color'],\n",
    "            marker=meta['marker'],\n",
    "            linewidth=2.5,\n",
    "            markersize=8,\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f\"{metric} Performance Across SNR Levels\", fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('SNR (dB)', fontsize=14)\n",
    "    ax.set_ylabel(metric, fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='best', frameon=True, shadow=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = figures_dir / \"gtcrn_ss_wf_comparison.png\"\n",
    "fig.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
    "print(f\"\\n✓ Plot saved: {plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HEATMAP: Performance by Noise Type and SNR (if available)\n",
    "# ============================================================================\n",
    "\n",
    "if 'noise_type' in full_results.columns and full_results['noise_type'].notna().any():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING HEATMAPS BY NOISE TYPE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    noise_types = full_results['noise_type'].dropna().unique()\n",
    "    \n",
    "    for metric in metrics_of_interest:\n",
    "        fig, axes = plt.subplots(1, len(catalog), figsize=(6*len(catalog), 5), sharey=True)\n",
    "        \n",
    "        if len(catalog) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (exp_name, meta) in enumerate(catalog.items()):\n",
    "            exp_data = full_results[full_results['experiment'] == exp_name]\n",
    "            \n",
    "            if exp_data.empty:\n",
    "                continue\n",
    "            \n",
    "            pivot = exp_data.pivot_table(\n",
    "                values=metric,\n",
    "                index='noise_type',\n",
    "                columns='SNR',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            sns.heatmap(\n",
    "                pivot,\n",
    "                annot=True,\n",
    "                fmt='.3f',\n",
    "                cmap='YlGnBu',\n",
    "                ax=axes[idx],\n",
    "                cbar_kws={'label': metric}\n",
    "            )\n",
    "            axes[idx].set_title(f\"{meta['label']}\", fontsize=14, fontweight='bold')\n",
    "            axes[idx].set_xlabel('SNR (dB)')\n",
    "            axes[idx].set_ylabel('Noise Type' if idx == 0 else '')\n",
    "        \n",
    "        plt.suptitle(f\"{metric} by Noise Type and SNR\", fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        heatmap_path = figures_dir / f\"{metric}_heatmap_by_noise.png\"\n",
    "        fig.savefig(heatmap_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✓ Heatmap saved: {heatmap_path}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DIRECT COMPARISON: SS vs WF at Low SNRs\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIRECT COMPARISON: SPECTRAL SUBTRACTION vs WIENER FILTER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "low_snr_data = summary_by_snr[summary_by_snr['SNR'].isin([-5, 0])]\n",
    "ss_data = low_snr_data[low_snr_data['experiment'] == 'GTCRN_SS']\n",
    "wf_data = low_snr_data[low_snr_data['experiment'] == 'GTCRN_WF']\n",
    "gtcrn_data = low_snr_data[low_snr_data['experiment'] == 'GTCRN']\n",
    "\n",
    "comparison = pd.merge(\n",
    "    ss_data[['SNR'] + metrics_of_interest],\n",
    "    wf_data[['SNR'] + metrics_of_interest],\n",
    "    on='SNR',\n",
    "    suffixes=('_SS', '_WF')\n",
    ")\n",
    "\n",
    "comparison = pd.merge(\n",
    "    comparison,\n",
    "    gtcrn_data[['SNR'] + metrics_of_interest],\n",
    "    on='SNR'\n",
    ")\n",
    "\n",
    "for metric in metrics_of_interest:\n",
    "    comparison[f'{metric}_SS_advantage'] = comparison[f'{metric}_SS'] - comparison[f'{metric}_WF']\n",
    "    comparison[f'{metric}_SS_vs_GTCRN'] = comparison[f'{metric}_SS'] - comparison[metric]\n",
    "    comparison[f'{metric}_WF_vs_GTCRN'] = comparison[f'{metric}_WF'] - comparison[metric]\n",
    "\n",
    "print(\"\\nAt -5dB and 0dB:\")\n",
    "display(comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: SS Advantage Over WF (Positive = SS Wins)\")\n",
    "print(\"=\"*80)\n",
    "for metric in metrics_of_interest:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  -5dB: {comparison[comparison['SNR']==-5][f'{metric}_SS_advantage'].values[0]:.4f}\")\n",
    "    print(f\"   0dB: {comparison[comparison['SNR']==0][f'{metric}_SS_advantage'].values[0]:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_dir = figures_dir / \"tables\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary_by_snr.to_csv(output_dir / \"summary_by_snr.csv\", index=False)\n",
    "overall_mean.to_csv(output_dir / \"overall_mean.csv\", index=False)\n",
    "pct_vs_noise.to_csv(output_dir / \"percentage_gains_vs_noise.csv\", index=False)\n",
    "pct_vs_gtcrn.to_csv(output_dir / \"percentage_gains_vs_gtcrn.csv\", index=False)\n",
    "best_table.to_csv(output_dir / \"best_config_per_metric_snr.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ All tables exported to: {output_dir}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
